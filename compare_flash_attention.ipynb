{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d46a1a4-d33a-4cf4-b1d2-1f367bedb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import math\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _fwd_kernel_og(Q, K, V, sm_scale,  #\n",
    "                L,  #\n",
    "                Out,  #\n",
    "                stride_qm, stride_qk,  #\n",
    "                stride_kn, stride_kk,  #\n",
    "                stride_vn, stride_vk,  #\n",
    "                stride_om, stride_on,  #\n",
    "                N_CTX,  #\n",
    "                BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,  #\n",
    "                BLOCK_N: tl.constexpr,  #\n",
    "                IS_CAUSAL: tl.constexpr  #\n",
    "                ):\n",
    "    start_m = tl.program_id(0)\n",
    "\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K,\n",
    "        shape=(BLOCK_DMODEL, N_CTX),\n",
    "        strides=(stride_kk, stride_kn),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_DMODEL, BLOCK_N),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_vn, stride_vk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_N, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    # initialize offsets\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    # initialize pointer  to m and l\n",
    "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
    "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "\n",
    "    qk_scale = sm_scale * 1.44269504\n",
    "    # load q: it will stay in SRAM throughout\n",
    "\n",
    "    offs_k = tl.arange(0, BLOCK_DMODEL)\n",
    "    Q_ptrs = Q + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n",
    "    q = tl.load(Q_ptrs)\n",
    "\n",
    "    q = (q * qk_scale).to(K.dtype.element_ty)\n",
    "    lo = 0\n",
    "    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n",
    "    for start_n in range(lo, hi, BLOCK_N):\n",
    "        # -- load k, v --\n",
    "        k = tl.load(K_block_ptr)\n",
    "        v = tl.load(V_block_ptr)\n",
    "        # -- compute qk ---\n",
    "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "        if IS_CAUSAL:\n",
    "            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n",
    "        qk += tl.dot(q, k)\n",
    "        # -- compute scaling constant ---\n",
    "        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n",
    "        alpha = tl.math.exp2(m_i - m_i_new)\n",
    "        p = tl.math.exp2(qk - m_i_new[:, None])\n",
    "        # -- scale and update acc --\n",
    "        acc *= alpha[:, None]\n",
    "        acc += tl.dot(p.to(V.dtype.element_ty), v)\n",
    "        # -- update m_i and l_i --\n",
    "        l_i = l_i * alpha + tl.sum(p, 1)\n",
    "        m_i = m_i_new\n",
    "        # update pointers\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n",
    "    # write back l and m\n",
    "    acc = acc / l_i[:, None]\n",
    "    l_ptrs = L + N_CTX + offs_m\n",
    "    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n",
    "    # write back O\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=Out,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_om, stride_on),\n",
    "        offsets=(start_m * BLOCK_M, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    # O_ptrs = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n",
    "    tl.store(O_block_ptr, acc.to(K.dtype.element_ty))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _bwd_preprocess_og(\n",
    "    Out,\n",
    "    DO,\n",
    "    Delta,\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    D_HEAD: tl.constexpr,\n",
    "):\n",
    "    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    off_n = tl.arange(0, D_HEAD)\n",
    "    # load\n",
    "    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n",
    "    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n",
    "    # compute\n",
    "    delta = tl.sum(o * do, axis=1)\n",
    "    # write-back\n",
    "    tl.store(Delta + off_m, delta)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _bwd_kernel_one_col_block_og(Q, K, V, sm_scale, qk_scale,  #\n",
    "                          Out, DO,  #\n",
    "                          DQ, DK, DV,  #\n",
    "                          L,  #\n",
    "                          D,  #\n",
    "                          Q_block_ptr, K_block_ptr, V_block_ptr,  #\n",
    "                          DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,  #\n",
    "                          stride_dqa, stride_qm, stride_qk,  #\n",
    "                          stride_kn, stride_kk,  #\n",
    "                          stride_vn, stride_vk,  #\n",
    "                          N_CTX,  #\n",
    "                          start_n, num_block,  #\n",
    "                          BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,  #\n",
    "                          BLOCK_N: tl.constexpr,  #\n",
    "                          ):\n",
    "\n",
    "    lo = 0\n",
    "    Q_block_ptr = tl.advance(Q_block_ptr, (lo, 0))\n",
    "    K_block_ptr = tl.advance(K_block_ptr, (start_n * BLOCK_M, 0))\n",
    "    V_block_ptr = tl.advance(V_block_ptr, (start_n * BLOCK_M, 0))\n",
    "    DO_block_ptr = tl.advance(DO_block_ptr, (lo, 0))\n",
    "    DQ_block_ptr = tl.advance(DQ_block_ptr, (lo, 0))\n",
    "    DK_block_ptr = tl.advance(DK_block_ptr, (start_n * BLOCK_M, 0))\n",
    "    DV_block_ptr = tl.advance(DV_block_ptr, (start_n * BLOCK_M, 0))\n",
    "\n",
    "    # initialize row/col offsets\n",
    "    offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_m = tl.arange(0, BLOCK_N)\n",
    "    # pointer to row-wise quantities in value-like data\n",
    "    D_ptrs = D\n",
    "    l_ptrs = L\n",
    "    # initialize dv amd dk\n",
    "    dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "    dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "    # k and v stay in SRAM throughout\n",
    "    k = tl.load(K_block_ptr)\n",
    "    v = tl.load(V_block_ptr)\n",
    "    # loop over rows\n",
    "    for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n",
    "        offs_m_curr = start_m + offs_m\n",
    "        # load q, k, v, do on-chip\n",
    "        q = tl.load(Q_block_ptr)\n",
    "        # recompute p = softmax(qk, dim=-1).T\n",
    "        # NOTE: `do` is pre-divided by `l`; no normalization here\n",
    "\n",
    "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "        qk += tl.dot(q, tl.trans(k))\n",
    "        qk *= qk_scale\n",
    "        l_i = tl.load(l_ptrs + offs_m_curr)\n",
    "        p = tl.math.exp2(qk - l_i[:, None])\n",
    "        # compute dv\n",
    "        do = tl.load(DO_block_ptr)\n",
    "        dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n",
    "        # compute dp = dot(v, do)\n",
    "        Di = tl.load(D_ptrs + offs_m_curr)\n",
    "        # dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n",
    "        dp = tl.dot(do, tl.trans(v))\n",
    "        # compute ds = p * (dp - delta[:, None])\n",
    "        ds = (p * (dp - Di[:, None]) * sm_scale).to(Q.dtype.element_ty)\n",
    "        # compute dk = dot(ds.T, q)\n",
    "        dk += tl.dot(tl.trans(ds), q)\n",
    "        # compute dq\n",
    "        dq = tl.load(DQ_block_ptr)\n",
    "        dq += tl.dot(ds, k)\n",
    "        tl.store(DQ_block_ptr, dq.to(Q.dtype.element_ty))\n",
    "\n",
    "\n",
    "        # increment pointers\n",
    "        DQ_block_ptr = tl.advance(DQ_block_ptr, (BLOCK_M, 0))\n",
    "        Q_block_ptr = tl.advance(Q_block_ptr, (BLOCK_M, 0))\n",
    "        DO_block_ptr = tl.advance(DO_block_ptr, (BLOCK_M, 0))\n",
    "    # write-back\n",
    "    tl.store(DV_block_ptr, dv.to(V.dtype.element_ty))\n",
    "    tl.store(DK_block_ptr, dk.to(K.dtype.element_ty))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _bwd_kernel_og(Q, K, V, sm_scale,  #\n",
    "        Out, DO,  #\n",
    "        DQ, DK, DV,  #\n",
    "        L,  #\n",
    "        D,  #\n",
    "        stride_dqa, stride_qm, stride_qk,  #\n",
    "        stride_kn, stride_kk,  #\n",
    "        stride_vn, stride_vk,  #\n",
    "        N_CTX,  #\n",
    "        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,  #\n",
    "        BLOCK_N: tl.constexpr,  #\n",
    "        ):\n",
    "    qk_scale = sm_scale * 1.44269504\n",
    "\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_qm, stride_qk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_kn, stride_kk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_vn, stride_vk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    DO_block_ptr = tl.make_block_ptr(\n",
    "        base=DO,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_qm, stride_qk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    DQ_block_ptr = tl.make_block_ptr(\n",
    "        base=DQ,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_qm, stride_qk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    DK_block_ptr = tl.make_block_ptr(\n",
    "        base=DK,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_kn, stride_kk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    DV_block_ptr = tl.make_block_ptr(\n",
    "        base=DV,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_vn, stride_vk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    num_block_n = tl.cdiv(N_CTX, BLOCK_N)\n",
    "    for start_n in range(0, num_block_n):\n",
    "        _bwd_kernel_one_col_block_og(Q, K, V, sm_scale, qk_scale, Out, DO,  #\n",
    "                                  DQ, DK, DV,  #\n",
    "                                  L,  #\n",
    "                                  D,  #\n",
    "                                  Q_block_ptr, K_block_ptr, V_block_ptr,  #\n",
    "                                  DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,  #\n",
    "                                  stride_dqa, stride_qm, stride_qk,  #\n",
    "                                  stride_kn, stride_kk,  #\n",
    "                                  stride_vn, stride_vk,  #\n",
    "                                  N_CTX,  #\n",
    "                                  start_n, num_block_n,  #\n",
    "                                  BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,  #\n",
    "                                  BLOCK_N=BLOCK_N,  #\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class _attention_og(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal, sm_scale, sequence_parallel=False):\n",
    "        # only support for Ampere now\n",
    "        capability = torch.cuda.get_device_capability()\n",
    "        if capability[0] < 8:\n",
    "            raise RuntimeError(\"Flash attention currently only supported for compute capability >= 80\")\n",
    "        BLOCK_M = 64\n",
    "        BLOCK_N = 64\n",
    "        # shape constraints\n",
    "        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n",
    "        assert Lq == Lk and Lk == Lv\n",
    "        assert Lk in {16, 32, 64, 128}\n",
    "        o = torch.empty_like(q)\n",
    "        grid = (triton.cdiv(q.shape[0], BLOCK_M), 1)\n",
    "        L = torch.empty((q.shape[0]), device=q.device, dtype=torch.float32)\n",
    "        num_warps = 4 if Lk <= 64 else 8\n",
    "        # print(grid)\n",
    "        _fwd_kernel_og[grid](\n",
    "            q, k, v, sm_scale,  #\n",
    "            L,  #\n",
    "            o,  #\n",
    "            q.stride(0), q.stride(1),  #\n",
    "            k.stride(0), k.stride(1), #\n",
    "            v.stride(0), v.stride(1),  #\n",
    "            o.stride(0), o.stride(1),  #\n",
    "            q.shape[0],  #\n",
    "            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,  #\n",
    "            IS_CAUSAL=causal,  #\n",
    "            num_warps=num_warps,  #\n",
    "            num_stages=4  #\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(q, k, v, o, L)\n",
    "        ctx.grid = grid\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.BLOCK_DMODEL = Lk\n",
    "        ctx.causal = causal\n",
    "        ctx.sequence_parallel = sequence_parallel\n",
    "        return o\n",
    "\n",
    "    @staticmethod\n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "        BLOCK = 64\n",
    "        q, k, v, o, L = ctx.saved_tensors\n",
    "        sequence_parallel = ctx.sequence_parallel\n",
    "        seq_len_kv = k.shape[0]\n",
    "        do = do.contiguous()\n",
    "\n",
    "        dq = torch.zeros_like(q, dtype=q.dtype)\n",
    "        dk = torch.empty_like(k)\n",
    "        dv = torch.empty_like(v)\n",
    "        delta = torch.empty_like(L)\n",
    "        _bwd_preprocess_og[(triton.cdiv(q.shape[0], BLOCK), )](\n",
    "            o,\n",
    "            do,\n",
    "            delta,\n",
    "            BLOCK_M=BLOCK,\n",
    "            D_HEAD=ctx.BLOCK_DMODEL,\n",
    "        )\n",
    "        _bwd_kernel_og[(1, )](\n",
    "            q, k, v, ctx.sm_scale,  #\n",
    "            o, do,  #\n",
    "            dq, dk, dv,  #\n",
    "            L,  #\n",
    "            delta,  #\n",
    "            o.numel(), q.stride(0), q.stride(1),  #\n",
    "            k.stride(0), k.stride(1), #\n",
    "            v.stride(0), v.stride(1), #\n",
    "            q.shape[0],  #\n",
    "            BLOCK_M=BLOCK, BLOCK_N=BLOCK,  #\n",
    "            BLOCK_DMODEL=ctx.BLOCK_DMODEL,  #\n",
    "            num_warps=8,  #\n",
    "            num_stages=1  #\n",
    "        )\n",
    "\n",
    "        if len(dq.shape) == 5:\n",
    "            dq = dq.sum(dim=0)\n",
    "        return dq, dk, dv, None, None, None\n",
    "\n",
    "\n",
    "attention_og = _attention_og.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e0369d-8d92-42d4-aece-57006633b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _bwd_kernel_one_col_block(Q, K, V, P, sm_scale, qk_scale,  #\n",
    "                              Out, DO,  #\n",
    "                              DQ, DK, DV,  #\n",
    "                              D,  #\n",
    "                              Q_block_ptr, K_block_ptr, V_block_ptr, P_block_ptr, #\n",
    "                              DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,  #\n",
    "                              stride_dqa, stride_qm, stride_qk,  #\n",
    "                              stride_kn, stride_kk,  #\n",
    "                              stride_vn, stride_vk,  #\n",
    "                              N_CTX,  #\n",
    "                              start_n, num_block,  #\n",
    "                              BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,  #\n",
    "                              BLOCK_N: tl.constexpr,  #\n",
    "                              ):\n",
    "    lo = 0\n",
    "    Q_block_ptr = tl.advance(Q_block_ptr, (lo, 0))\n",
    "    K_block_ptr = tl.advance(K_block_ptr, (start_n * BLOCK_M, 0))\n",
    "    V_block_ptr = tl.advance(V_block_ptr, (start_n * BLOCK_M, 0))\n",
    "    DO_block_ptr = tl.advance(DO_block_ptr, (lo, 0))\n",
    "    DQ_block_ptr = tl.advance(DQ_block_ptr, (lo, 0))\n",
    "    DK_block_ptr = tl.advance(DK_block_ptr, (start_n * BLOCK_M, 0))\n",
    "    DV_block_ptr = tl.advance(DV_block_ptr, (start_n * BLOCK_M, 0))\n",
    "    \n",
    "    P_block_ptr = tl.advance(P_block_ptr,(0,start_n*BLOCK_M))\n",
    "    \n",
    "    offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_m = tl.arange(0, BLOCK_N)\n",
    "    D_ptrs = D \n",
    "    dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "    dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "    k = tl.load(K_block_ptr)\n",
    "    v = tl.load(V_block_ptr)\n",
    "    \n",
    "    for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n",
    "        offs_m_curr = start_m + offs_m\n",
    "        # load q, k, v, do on-chip\n",
    "        q = tl.load(Q_block_ptr)\n",
    "        # Loading Attention scores on-chip\n",
    "        p = tl.load(P_block_ptr)\n",
    "        # compute dv\n",
    "        do = tl.load(DO_block_ptr)\n",
    "        dv += tl.dot(tl.trans(p), do)\n",
    "        # compute dp = dot(v, do)\n",
    "        Di = tl.load(D_ptrs + offs_m_curr)\n",
    "        # dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n",
    "        dp = tl.dot(do, tl.trans(v))\n",
    "        # compute ds = p * (dp - delta[:, None])\n",
    "        ds = (p * (dp - Di[:, None]) * sm_scale)\n",
    "        # compute dk = dot(ds.T, q)\n",
    "        dk += tl.dot(tl.trans(ds), q)\n",
    "        # compute dq\n",
    "        \n",
    "        dq = tl.load(DQ_block_ptr)\n",
    "        dq += tl.dot(ds, k)\n",
    "        tl.store(DQ_block_ptr, dq)\n",
    "\n",
    "        # increment pointers\n",
    "        DQ_block_ptr = tl.advance(DQ_block_ptr, (BLOCK_M, 0))\n",
    "        Q_block_ptr = tl.advance(Q_block_ptr, (BLOCK_M, 0))\n",
    "        DO_block_ptr = tl.advance(DO_block_ptr, (BLOCK_M, 0))\n",
    "        P_block_ptr = tl.advance(P_block_ptr,(BLOCK_M,0))\n",
    "\n",
    "    # write-back\n",
    "    tl.store(DV_block_ptr, dv.to(V.dtype.element_ty))\n",
    "    tl.store(DK_block_ptr, dk.to(K.dtype.element_ty))\n",
    "    \n",
    "@triton.jit\n",
    "def _bwd_kernel(Q, K, V, P, sm_scale,  #\n",
    "                Out, DO,  #\n",
    "                DQ, DK, DV,  #\n",
    "                D,  #\n",
    "                stride_dqa, stride_qm, stride_qk,  #\n",
    "                stride_kn, stride_kk,  #\n",
    "                stride_vn, stride_vk,  #\n",
    "                stride_pm, stride_pn,  #\n",
    "                N_CTX,  #\n",
    "                BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,  #\n",
    "                BLOCK_N: tl.constexpr,  #\n",
    "                ):\n",
    "    qk_scale = sm_scale * 1.44269504\n",
    "\n",
    "    \n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_qm, stride_qk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_kn, stride_kk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_vn, stride_vk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    P_block_ptr = tl.make_block_ptr(\n",
    "        base=P,\n",
    "        shape=(N_CTX, N_CTX),\n",
    "        strides=(stride_pm, stride_pn),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_M),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    DO_block_ptr = tl.make_block_ptr(\n",
    "        base=DO,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_qm, stride_qk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    DQ_block_ptr = tl.make_block_ptr(\n",
    "            base=DQ,\n",
    "            shape=(N_CTX, BLOCK_DMODEL),\n",
    "            strides=(stride_qm, stride_qk),\n",
    "            offsets=(0, 0),\n",
    "            block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "    DK_block_ptr = tl.make_block_ptr(\n",
    "        base=DK,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_kn, stride_kk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    DV_block_ptr = tl.make_block_ptr(\n",
    "        base=DV,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_vn, stride_vk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    num_block_n = tl.cdiv(N_CTX, BLOCK_N)\n",
    "    for start_n in range(0, num_block_n):\n",
    "            _bwd_kernel_one_col_block(Q, K, V, P, sm_scale, qk_scale, Out, DO,  #\n",
    "                                      DQ, DK, DV,  #\n",
    "                                      D,  #\n",
    "                                      Q_block_ptr, K_block_ptr, V_block_ptr, P_block_ptr, #\n",
    "                                      DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,  #\n",
    "                                      stride_dqa, stride_qm, stride_qk,  #\n",
    "                                      stride_kn, stride_kk,  #\n",
    "                                      stride_vn, stride_vk,  #\n",
    "                                      N_CTX,  #\n",
    "                                      start_n, num_block_n,  #\n",
    "                                      BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,  #\n",
    "                                      BLOCK_N=BLOCK_N,  #\n",
    "                                      )\n",
    "\n",
    "    \n",
    "@triton.jit\n",
    "def _attn_fwd(Q, K, V, sm_scale, P, O, #\n",
    "              stride_qm, stride_qk,  #\n",
    "              stride_kn, stride_kk,  #\n",
    "              stride_vn, stride_vk,  #\n",
    "              stride_pm, stride_pn,\n",
    "              stride_om, stride_on,#\n",
    "              N_CTX: tl.constexpr,  #\n",
    "              BLOCK_M: tl.constexpr,  #\n",
    "              BLOCK_DMODEL: tl.constexpr,  #\n",
    "              BLOCK_N: tl.constexpr,  #\n",
    "              ):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_qm, stride_qk),\n",
    "        offsets=(pid * BLOCK_M, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K,\n",
    "        shape=(BLOCK_DMODEL, N_CTX),\n",
    "        strides=(stride_kk, stride_kn),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_DMODEL, BLOCK_N),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    P_block_ptr = tl.make_block_ptr(\n",
    "        base=P,\n",
    "        shape=(N_CTX, N_CTX),\n",
    "        strides=(stride_pm, stride_pn),\n",
    "        offsets=(pid * BLOCK_M, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_N),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_vn, stride_vk),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_N, BLOCK_DMODEL),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_om, stride_on),\n",
    "        offsets=(pid * BLOCK_M, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0))\n",
    "    \n",
    "    # acc = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "    Out = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
    "    qk_scale = sm_scale * 1.44269504\n",
    "    \n",
    "    q = tl.load(Q_block_ptr)\n",
    "    q = (q * qk_scale)\n",
    "    lo, hi = 0, N_CTX \n",
    "    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
    "    for start_n in range(lo, hi, BLOCK_N):\n",
    "        # -- load k, v --\n",
    "        k = tl.load(K_block_ptr)\n",
    "        # -- compute qk ---\n",
    "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "\n",
    "        qk += tl.dot(q, k)\n",
    "        # -- compute scaling constant ---\n",
    "        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n",
    "        alpha = tl.math.exp2(m_i - m_i_new)\n",
    "        p = tl.math.exp2(qk - m_i_new[:, None])\n",
    "        l_i = l_i * alpha + tl.sum(p, 1)\n",
    "        m_i = m_i_new\n",
    "        # update pointers\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n",
    "    \n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K,\n",
    "        shape=(BLOCK_DMODEL, N_CTX),\n",
    "        strides=(stride_kk, stride_kn),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_DMODEL, BLOCK_N),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    lo, hi = 0, N_CTX \n",
    "    for start_n in range(lo, hi, BLOCK_N):\n",
    "        # -- load k, v --\n",
    "        k = tl.load(K_block_ptr)\n",
    "        v = tl.load(V_block_ptr)\n",
    "        # -- compute qk ---\n",
    "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "        qk += tl.dot(q, k)\n",
    "        # -- compute scaling constant ---\n",
    "        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n",
    "        alpha = tl.math.exp2(m_i - m_i_new)\n",
    "        p = tl.math.exp2(qk - m_i_new[:, None])\n",
    "        # -- scale and update acc --\n",
    "        # acc *= alpha[:, None]\n",
    "        acc += tl.dot(p, v)\n",
    "        # -- update m_i and l_i --\n",
    "        m_i = m_i_new\n",
    "        # update pointers\n",
    "        p = p/ l_i[:, None]\n",
    "        # qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "        tl.store(P_block_ptr, p)\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n",
    "        P_block_ptr = tl.advance(P_block_ptr, (0, BLOCK_N))\n",
    "    # # write back O\n",
    "    acc = acc / l_i[:, None]\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O,\n",
    "        shape=(N_CTX, BLOCK_DMODEL),\n",
    "        strides=(stride_om, stride_on),\n",
    "        offsets=(pid * BLOCK_M, 0),\n",
    "        block_shape=(BLOCK_M, BLOCK_DMODEL),\n",
    "        order=(1, 0))\n",
    "    tl.store(O_block_ptr, acc)\n",
    "    \n",
    "@triton.jit\n",
    "def _bwd_preprocess(\n",
    "    Out,\n",
    "    DO,\n",
    "    Delta,\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    D_HEAD: tl.constexpr,\n",
    "):\n",
    "    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    off_n = tl.arange(0, D_HEAD)\n",
    "    # load\n",
    "    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n",
    "    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n",
    "    # compute\n",
    "    delta = tl.sum(o * do, axis=1)\n",
    "    # write-back\n",
    "    tl.store(Delta + off_m, delta)\n",
    "\n",
    "class _attention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal, sm_scale):\n",
    "        # shape constraints\n",
    "        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n",
    "        # when v is in float8_e5m2 it is transposed.\n",
    "        assert Lq == Lk and (Lk == Lv or v.dtype == torch.float8_e5m2)\n",
    "        p = torch.randn((q.shape[0], q.shape[0]), dtype=dtype, device=\"cuda\", requires_grad=True)\n",
    "        out = torch.randn((q.shape[0], Lk), dtype=dtype, device=\"cuda\", requires_grad=True)\n",
    "        BLOCK_M = 64\n",
    "        BLOCK_N = 64 \n",
    "        grid = (triton.cdiv(q.shape[0], BLOCK_M), 1)\n",
    "        # print(\"grid: \", grid)\n",
    "        _attn_fwd[grid](\n",
    "            q, k, v, sm_scale, p, out, #\n",
    "            q.stride(0), q.stride(1),  #\n",
    "            k.stride(0), k.stride(1),  #\n",
    "            v.stride(0), v.stride(1), #\n",
    "            p.stride(0), p.stride(1),  #\n",
    "            out.stride(0), out.stride(1),\n",
    "            N_CTX=q.shape[0],  #\n",
    "            BLOCK_M=BLOCK_M,  #\n",
    "            BLOCK_N=BLOCK_N,  #\n",
    "            BLOCK_DMODEL=Lk  #\n",
    "        )\n",
    "        ctx.save_for_backward(q, k, v, out, p)\n",
    "        ctx.grid = grid\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.BLOCK_DMODEL = Lk\n",
    "        return out\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "\n",
    "        BLOCK = 64\n",
    "        q, k, v, out, p = ctx.saved_tensors\n",
    "        seq_len_kv = k.shape[0]\n",
    "        do = do.contiguous()\n",
    "        dq = torch.zeros_like(q, dtype=q.dtype)\n",
    "        dk = torch.empty_like(k)\n",
    "        dv = torch.empty_like(v)\n",
    "        delta = torch.empty((q.shape[0]), device=q.device, dtype=torch.float32)\n",
    "        _bwd_preprocess[(triton.cdiv(q.shape[0], BLOCK), )](\n",
    "            o,\n",
    "            do,\n",
    "            delta,\n",
    "            BLOCK_M=BLOCK,\n",
    "            D_HEAD=ctx.BLOCK_DMODEL,\n",
    "        )\n",
    "\n",
    "        _bwd_kernel[(1,)](\n",
    "            q, k, v, p, ctx.sm_scale,  #\n",
    "            o, do,  #\n",
    "            dq, dk, dv,  #\n",
    "            delta,  #\n",
    "            o.numel(), q.stride(0), q.stride(1), #\n",
    "            k.stride(0), k.stride(1),  #\n",
    "            v.stride(0), v.stride(1),\n",
    "            p.stride(0), p.stride(1), #\n",
    "            q.shape[0],  #\n",
    "            BLOCK_M=BLOCK, BLOCK_N=BLOCK,  #\n",
    "            BLOCK_DMODEL=ctx.BLOCK_DMODEL,  #\n",
    "            num_warps=8,  #\n",
    "            num_stages=2  #\n",
    "        )\n",
    "\n",
    "        \n",
    "        if len(dq.shape) == 5:\n",
    "            dq = dq.sum(dim=0)\n",
    "        return dq, dk, dv, None, None, None\n",
    "\n",
    "attention = _attention.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8584ad89-12a0-40d0-8159-57bbad0f475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, N_CTX, D_HEAD, attn_mask=None, causal=False):\n",
    "    sm_scale = 1.3\n",
    "    dout = torch.randn_like(q)\n",
    "    # reference implementation\n",
    "    p = torch.matmul(q, k.transpose(0, 1))* sm_scale\n",
    "\n",
    "    p = torch.softmax(p.float(), dim=-1)\n",
    "    # p = torch.exp(p)\n",
    "    ref_out = torch.matmul(p, v)\n",
    "    return p, ref_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6aebf1-5fbb-49fd-aa79-e611d320517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CTX, D_HEAD =  512, 32\n",
    "dtype = torch.float32\n",
    "sm_scale = 1.3\n",
    "causal = False\n",
    "q = torch.randn((N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n",
    "k = torch.randn((N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n",
    "v = torch.randn((N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n",
    "dout = torch.randn_like(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf0bfed-60dd-412e-9ad5-c0a325bbd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_og = lambda: attention_og(q, k, v, causal, sm_scale)\n",
    "o_og = fn_og()\n",
    "o_og.backward(dout)\n",
    "tri_dv_og, v.grad = v.grad.clone(), None\n",
    "tri_dk_og, k.grad = k.grad.clone(), None\n",
    "tri_dq_og, q.grad = q.grad.clone(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28be490c-0f30-4f5d-97f4-2758b9a56ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = lambda: attention(q, k, v, causal, sm_scale)\n",
    "o = fn()\n",
    "\n",
    "o.backward(dout)\n",
    "tri_dv, v.grad = v.grad.clone(), None\n",
    "tri_dk, k.grad = k.grad.clone(), None\n",
    "tri_dq, q.grad = q.grad.clone(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119bfcb0-d943-414e-9b74-31940bd9f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weight, output = scaled_dot_product_attention(q,k,v, N_CTX, D_HEAD)\n",
    "output.backward(dout)\n",
    "ref_dv, v.grad = v.grad.clone(), None\n",
    "ref_dk, k.grad = k.grad.clone(), None\n",
    "ref_dq, q.grad = q.grad.clone(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d0dda0-eac4-45c0-a95d-0cd7ec84a715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7522e-02, -7.7869e-02, -1.3638e-01,  ...,  9.6032e-02,\n",
       "          4.3343e-02, -1.1753e-01],\n",
       "        [-8.8337e-03,  1.4211e-02, -1.8581e-03,  ...,  8.4664e-04,\n",
       "         -2.6901e-03, -2.6090e-02],\n",
       "        [-3.6214e-01,  4.9493e-02,  2.6871e-01,  ..., -2.2330e-01,\n",
       "         -1.1101e-01, -1.5554e-01],\n",
       "        ...,\n",
       "        [-1.2211e-03, -3.6267e-03,  7.9654e-04,  ...,  6.9685e-04,\n",
       "          2.7216e-04,  2.2713e-03],\n",
       "        [ 5.3058e-01,  2.8045e+00, -8.5060e-01,  ..., -1.7564e+00,\n",
       "         -1.5460e+00, -9.7452e-01],\n",
       "        [ 3.5949e-03,  4.7178e-03, -3.1673e-03,  ..., -4.5652e-04,\n",
       "          9.3315e-04,  1.0591e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a6ecba-b12e-4530-8781-d367d8e6bfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7560e-02, -7.8482e-02, -1.3677e-01,  ...,  9.6692e-02,\n",
       "          4.3316e-02, -1.1781e-01],\n",
       "        [-8.8613e-03,  1.4235e-02, -1.8245e-03,  ...,  8.8070e-04,\n",
       "         -2.7059e-03, -2.6104e-02],\n",
       "        [-3.6234e-01,  4.9521e-02,  2.6870e-01,  ..., -2.2340e-01,\n",
       "         -1.1106e-01, -1.5562e-01],\n",
       "        ...,\n",
       "        [-1.2264e-03, -3.6353e-03,  7.9951e-04,  ...,  6.9646e-04,\n",
       "          2.7357e-04,  2.2764e-03],\n",
       "        [ 5.2933e-01,  2.8014e+00, -8.5067e-01,  ..., -1.7558e+00,\n",
       "         -1.5430e+00, -9.7408e-01],\n",
       "        [ 3.6019e-03,  4.7182e-03, -3.1674e-03,  ..., -4.5996e-04,\n",
       "          9.3456e-04,  1.0623e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2b8da-e49a-426c-92c6-49c14e914b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
